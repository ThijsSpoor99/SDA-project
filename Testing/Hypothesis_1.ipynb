{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bilt = pd.read_csv('Data/de_bilt_weather.csv')\n",
    "\n",
    "# Shift days so that the first day starts from zero\n",
    "df_bilt['days'] = df_bilt['days'] - df_bilt['days'].min()\n",
    "\n",
    "variables = ['cloud_cover', 'wind_speed', 'wind_gust', 'humidity',\n",
    "             'pressure', 'global_radiation', 'precipitation', 'sunshine', \n",
    "             'temp_mean', 'temp_min', 'temp_max'] \n",
    "\n",
    "n_var = len(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training on year of data, testing on 10 days\n",
    "train_range = 365\n",
    "test_range = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data with shape (n_var, train_range+test_range)\n",
    "data = []\n",
    "for variable_name in variables:\n",
    "    data.append(df_bilt[variable_name][:train_range + test_range])\n",
    "data = np.vstack(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting training and test sets\n",
    "train_data = data[:, :train_range]   \n",
    "test_data = data[:, train_range:]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cost function for the optimizer\n",
    "def var_initial_norm(params, data, means, n_var):\n",
    "    c = params[:n_var]\n",
    "    matrix = params[n_var:].reshape(n_var, n_var)\n",
    "    prediction = c[:, None] + matrix @ data[:, :-1]\n",
    "    residuals = data[:, 1:] - prediction\n",
    "    return np.linalg.norm(residuals / means[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize c and M to zero\n",
    "params = np.zeros(n_var + n_var**2)\n",
    "means = np.mean(train_data, axis=1)\n",
    "\n",
    "#Scipy's optimizer fits model parameters by minimizing cost function\n",
    "result = scipy.optimize.minimize(var_initial_norm, params, method='Powell', args=(train_data, means, n_var))\n",
    "\n",
    "#Represents the fitted intercepts and coefficient matrix\n",
    "c_var = result.x[:n_var]\n",
    "M_var = result.x[n_var:].reshape(n_var, n_var)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = train_data[:, 1:] - (c_var[:, None] + M_var @ train_data[:, :-1])\n",
    "\n",
    "#Generating uncertainty in predictions\n",
    "std_var = np.std(residuals, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Produces predictions for next day given today, fits model plus normally distributed noise for uncertainty.\n",
    "def weather_var(x, c, M, std):\n",
    "    return c + M @ x + np.random.normal(0, std)\n",
    "\n",
    "#Time indices for training and testing\n",
    "t_train_data = np.arange(train_range)\n",
    "t_test_data = np.arange(train_range, train_range + test_range)\n",
    "\n",
    "#Number of simulation runs to capture uncertainty\n",
    "prediction_list = []\n",
    "n_predictions = 1000\n",
    "\n",
    "for j in range(n_predictions):\n",
    "    #First test day is initial condition\n",
    "    prediction = [test_data[:, 0]]\n",
    "    #Predict forward for remaining test days\n",
    "    for _ in t_test_data[1:]:\n",
    "        prediction.append(weather_var(prediction[-1], c_var, M_var, std_var))\n",
    "    prediction = np.array(prediction).T\n",
    "    prediction_list.append(prediction)\n",
    "\n",
    "#List of predictions converting to array with shape (n_predictions, n_var, test_range)\n",
    "prediction_matrix = np.array(prediction_list)\n",
    "mean = np.mean(prediction_matrix, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Placing mean of predictions in a DF with variables as columns and days as rows\n",
    "predictions_df = pd.DataFrame(mean.T, columns=variables)\n",
    "\n",
    "#Correlation matrix of the VARs predictions\n",
    "corr_matrix = predictions_df.corr()\n",
    "\n",
    "#Correlation threshold\n",
    "threshold = 0.5\n",
    "\n",
    "# Identifying pairs of variables with high correlation\n",
    "high_corr_pairs = []\n",
    "for i, var1 in enumerate(variables):\n",
    "    for var2 in variables[i + 1:]:\n",
    "        corr_value = corr_matrix.loc[var1, var2]\n",
    "        if abs(corr_value) > threshold:\n",
    "            high_corr_pairs.append((var1, var2, corr_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dict from the test period of days, actual variable, and var_forecast of variable\n",
    "days_test = df_bilt['days'].iloc[train_range:train_range + test_range].values\n",
    "data_dict = {\"days\": days_test}\n",
    "\n",
    "# Adding actual observed values for each variable\n",
    "for i, variable_name in enumerate(variables):\n",
    "    data_dict[f\"actual_{variable_name}\"] = test_data[i, :]\n",
    "\n",
    "#Adding VAR mean predictions for each variable\n",
    "for i, variable_name in enumerate(variables):\n",
    "    data_dict[f\"var_forecast_{variable_name}\"] = mean[i, :]\n",
    "\n",
    "df_var = pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Standalone LRM mean prediciton values \n",
    "files = [\n",
    "    'Testing/LRM_Data/cloud_cover.csv',\n",
    "    'Testing/LRM_Data/global_radiation.csv',\n",
    "    'Testing/LRM_Data/humidity.csv',\n",
    "    'Testing/LRM_Data/precipitation.csv',\n",
    "    'Testing/LRM_Data/pressure.csv',\n",
    "    'Testing/LRM_Data/sunshine.csv',\n",
    "    'Testing/LRM_Data/temp_max.csv',\n",
    "    'Testing/LRM_Data/temp_mean.csv',\n",
    "    'Testing/LRM_Data/temp_min.csv',\n",
    "    'Testing/LRM_Data/wind_gust.csv',\n",
    "    'Testing/LRM_Data/wind_speed.csv',\n",
    "]\n",
    "\n",
    "df_lrm_merged = None\n",
    "#Merging each LRM file on days\n",
    "for i, f in enumerate(files):\n",
    "    df_temp = pd.read_csv(f)\n",
    "    if i == 0:\n",
    "        df_lrm_merged = df_temp\n",
    "    else:\n",
    "        df_lrm_merged = pd.merge(df_lrm_merged, df_temp, on = 'days')\n",
    "\n",
    "#Merging VAR results with df_lrm_merged on days\n",
    "df_merged = pd.merge(df_var, df_lrm_merged, on='days', suffixes=(\"\", \"_lrm\"))\n",
    "\n",
    "# Rename VAR actual columns to standard names\n",
    "rename_dict = {}\n",
    "for var in variables:\n",
    "    old_name = f\"actual_{var}_var\"\n",
    "    new_name = f\"actual_{var}\"\n",
    "    if old_name in df_merged.columns:\n",
    "        rename_dict[old_name] = new_name\n",
    "\n",
    "df_merged.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "#Dropping duplicate actual coloumns\n",
    "for var in variables:\n",
    "    col_to_drop = f\"actual_{var}_lrm\"\n",
    "    if col_to_drop in df_merged.columns:\n",
    "        df_merged.drop(columns=col_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def norm_cdf(z):\n",
    "    #cdf of the standard normal variable at point z using the error function (erf).\n",
    "    return 0.5 * (1 + math.erf(z / math.sqrt(2)))\n",
    "\n",
    "def diebold_mariano_test(d, alternative = 'two-sided'):\n",
    "    #Converting np array for vectorized operatoring\n",
    "    d = np.array(d)\n",
    "    \n",
    "    #T is the number of observations of loss differentials in the data\n",
    "    T = d.size\n",
    "    d_bar = np.mean(d)\n",
    "    lag = 1\n",
    "\n",
    "    #gamma stores autocovariances for the loss differential\n",
    "    gamma = []\n",
    "    for k in range(T - 1):\n",
    "        gamma.append((d[k] - d_bar) * (d[k+1] - d_bar))\n",
    "    gamma = np.array(gamma)\n",
    "\n",
    "    #Accounting for potential autocorrelation in the differentials with the Newey-West long-run variance estimate for d.\n",
    "    var_d = (np.sum((d - d_bar)**2) + 2 * np.sum(gamma[:lag])) / T\n",
    "\n",
    "    #The Diebold-Mariano (DM) statistic:\n",
    "    dm_stat = d_bar / np.sqrt(var_d / T)\n",
    "\n",
    "    # two-sided test: p-value = 2 * (1 - cdf(|dm_stat|))\n",
    "    if alternative == 'two-sided':\n",
    "        p_value = 2 * (1 - norm_cdf(abs(dm_stat)))\n",
    "        # one-sided \"less\": p-value = cdf(dm_stat)\n",
    "    elif alternative == 'less':\n",
    "        p_value = norm_cdf(dm_stat)\n",
    "    # one-sided \"greater\": p-value = 1 - cdf(dm_stat)\n",
    "    elif alternative == 'greater':\n",
    "        p_value = 1 - norm_cdf(dm_stat)\n",
    "\n",
    "    return dm_stat, p_value\n",
    "\n",
    "def run_dm_test_for_variable(df, variable):\n",
    "    # Extract data for this variable\n",
    "    actual = df[f\"actual_{variable}\"].values\n",
    "    var_forecast = df[f\"var_forecast_{variable}\"].values\n",
    "    lrm_forecast = df[f\"lrm_forecast_{variable}\"].values\n",
    "    \n",
    "    #Find squared errors of predictions\n",
    "    var_errors = (actual - var_forecast)**2\n",
    "    lrm_errors = (actual - lrm_forecast)**2\n",
    "    \n",
    "    #Loss differential = LRM loss - VAR loss\n",
    "    d = lrm_errors - var_errors\n",
    "    \n",
    "    # Run the DM test\n",
    "    dm_stat, p_value = diebold_mariano_test(d, alternative='two-sided')\n",
    "    \n",
    "    return {\"variable\": variable, \"DM_stat\": dm_stat, \"p_value\": p_value}\n",
    "\n",
    "#Iterating for each variable\n",
    "results = []\n",
    "for var in variables:\n",
    "    res = run_dm_test_for_variable(df_merged, var)\n",
    "    results.append(res)\n",
    "\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the LRM R^2 file for presentation comparison\n",
    "df_r2 = pd.read_csv(\"Testing/LRM_Data/R_2.csv\", header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b9a40 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_b9a40_row0_col0, #T_b9a40_row0_col1, #T_b9a40_row0_col2, #T_b9a40_row0_col3, #T_b9a40_row0_col4, #T_b9a40_row1_col0, #T_b9a40_row1_col1, #T_b9a40_row1_col2, #T_b9a40_row1_col3, #T_b9a40_row1_col4, #T_b9a40_row2_col0, #T_b9a40_row2_col1, #T_b9a40_row2_col2, #T_b9a40_row2_col3, #T_b9a40_row2_col4, #T_b9a40_row3_col0, #T_b9a40_row3_col1, #T_b9a40_row3_col2, #T_b9a40_row3_col3, #T_b9a40_row3_col4, #T_b9a40_row4_col0, #T_b9a40_row4_col1, #T_b9a40_row4_col2, #T_b9a40_row4_col3, #T_b9a40_row4_col4, #T_b9a40_row5_col0, #T_b9a40_row5_col1, #T_b9a40_row5_col2, #T_b9a40_row5_col3, #T_b9a40_row5_col4, #T_b9a40_row6_col0, #T_b9a40_row6_col1, #T_b9a40_row6_col2, #T_b9a40_row6_col3, #T_b9a40_row6_col4, #T_b9a40_row7_col0, #T_b9a40_row7_col1, #T_b9a40_row7_col2, #T_b9a40_row7_col3, #T_b9a40_row7_col4, #T_b9a40_row8_col0, #T_b9a40_row8_col1, #T_b9a40_row8_col2, #T_b9a40_row8_col3, #T_b9a40_row8_col4, #T_b9a40_row9_col0, #T_b9a40_row9_col1, #T_b9a40_row9_col2, #T_b9a40_row9_col3, #T_b9a40_row9_col4, #T_b9a40_row10_col0, #T_b9a40_row10_col1, #T_b9a40_row10_col2, #T_b9a40_row10_col3, #T_b9a40_row10_col4 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b9a40\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_b9a40_level0_col0\" class=\"col_heading level0 col0\" >Variable</th>\n",
       "      <th id=\"T_b9a40_level0_col1\" class=\"col_heading level0 col1\" >DM_stat</th>\n",
       "      <th id=\"T_b9a40_level0_col2\" class=\"col_heading level0 col2\" >p_value</th>\n",
       "      <th id=\"T_b9a40_level0_col3\" class=\"col_heading level0 col3\" >LRM Training R^2 > 0.85</th>\n",
       "      <th id=\"T_b9a40_level0_col4\" class=\"col_heading level0 col4\" >VAR Correlation |r| > 0.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_b9a40_row0_col0\" class=\"data row0 col0\" >cloud_cover</td>\n",
       "      <td id=\"T_b9a40_row0_col1\" class=\"data row0 col1\" >0.736613</td>\n",
       "      <td id=\"T_b9a40_row0_col2\" class=\"data row0 col2\" >0.461358</td>\n",
       "      <td id=\"T_b9a40_row0_col3\" class=\"data row0 col3\" ></td>\n",
       "      <td id=\"T_b9a40_row0_col4\" class=\"data row0 col4\" >x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b9a40_row1_col0\" class=\"data row1 col0\" >wind_speed</td>\n",
       "      <td id=\"T_b9a40_row1_col1\" class=\"data row1 col1\" >1.849628</td>\n",
       "      <td id=\"T_b9a40_row1_col2\" class=\"data row1 col2\" >0.064367</td>\n",
       "      <td id=\"T_b9a40_row1_col3\" class=\"data row1 col3\" ></td>\n",
       "      <td id=\"T_b9a40_row1_col4\" class=\"data row1 col4\" >x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b9a40_row2_col0\" class=\"data row2 col0\" >wind_gust</td>\n",
       "      <td id=\"T_b9a40_row2_col1\" class=\"data row2 col1\" >0.894339</td>\n",
       "      <td id=\"T_b9a40_row2_col2\" class=\"data row2 col2\" >0.371141</td>\n",
       "      <td id=\"T_b9a40_row2_col3\" class=\"data row2 col3\" ></td>\n",
       "      <td id=\"T_b9a40_row2_col4\" class=\"data row2 col4\" >x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b9a40_row3_col0\" class=\"data row3 col0\" >humidity</td>\n",
       "      <td id=\"T_b9a40_row3_col1\" class=\"data row3 col1\" >3.174162</td>\n",
       "      <td id=\"T_b9a40_row3_col2\" class=\"data row3 col2\" >0.001503</td>\n",
       "      <td id=\"T_b9a40_row3_col3\" class=\"data row3 col3\" >x</td>\n",
       "      <td id=\"T_b9a40_row3_col4\" class=\"data row3 col4\" >x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b9a40_row4_col0\" class=\"data row4 col0\" >pressure</td>\n",
       "      <td id=\"T_b9a40_row4_col1\" class=\"data row4 col1\" >-3.635675</td>\n",
       "      <td id=\"T_b9a40_row4_col2\" class=\"data row4 col2\" >0.000277</td>\n",
       "      <td id=\"T_b9a40_row4_col3\" class=\"data row4 col3\" >x</td>\n",
       "      <td id=\"T_b9a40_row4_col4\" class=\"data row4 col4\" >x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b9a40_row5_col0\" class=\"data row5 col0\" >global_radiation</td>\n",
       "      <td id=\"T_b9a40_row5_col1\" class=\"data row5 col1\" >4.377473</td>\n",
       "      <td id=\"T_b9a40_row5_col2\" class=\"data row5 col2\" >0.000012</td>\n",
       "      <td id=\"T_b9a40_row5_col3\" class=\"data row5 col3\" >x</td>\n",
       "      <td id=\"T_b9a40_row5_col4\" class=\"data row5 col4\" >x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b9a40_row6_col0\" class=\"data row6 col0\" >precipitation</td>\n",
       "      <td id=\"T_b9a40_row6_col1\" class=\"data row6 col1\" >1.455811</td>\n",
       "      <td id=\"T_b9a40_row6_col2\" class=\"data row6 col2\" >0.145445</td>\n",
       "      <td id=\"T_b9a40_row6_col3\" class=\"data row6 col3\" ></td>\n",
       "      <td id=\"T_b9a40_row6_col4\" class=\"data row6 col4\" >x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b9a40_row7_col0\" class=\"data row7 col0\" >sunshine</td>\n",
       "      <td id=\"T_b9a40_row7_col1\" class=\"data row7 col1\" >2.833858</td>\n",
       "      <td id=\"T_b9a40_row7_col2\" class=\"data row7 col2\" >0.004599</td>\n",
       "      <td id=\"T_b9a40_row7_col3\" class=\"data row7 col3\" ></td>\n",
       "      <td id=\"T_b9a40_row7_col4\" class=\"data row7 col4\" >x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b9a40_row8_col0\" class=\"data row8 col0\" >temp_mean</td>\n",
       "      <td id=\"T_b9a40_row8_col1\" class=\"data row8 col1\" >-2.494775</td>\n",
       "      <td id=\"T_b9a40_row8_col2\" class=\"data row8 col2\" >0.012604</td>\n",
       "      <td id=\"T_b9a40_row8_col3\" class=\"data row8 col3\" >x</td>\n",
       "      <td id=\"T_b9a40_row8_col4\" class=\"data row8 col4\" >x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b9a40_row9_col0\" class=\"data row9 col0\" >temp_min</td>\n",
       "      <td id=\"T_b9a40_row9_col1\" class=\"data row9 col1\" >-1.381523</td>\n",
       "      <td id=\"T_b9a40_row9_col2\" class=\"data row9 col2\" >0.167118</td>\n",
       "      <td id=\"T_b9a40_row9_col3\" class=\"data row9 col3\" >x</td>\n",
       "      <td id=\"T_b9a40_row9_col4\" class=\"data row9 col4\" >x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b9a40_row10_col0\" class=\"data row10 col0\" >temp_max</td>\n",
       "      <td id=\"T_b9a40_row10_col1\" class=\"data row10 col1\" >-1.089384</td>\n",
       "      <td id=\"T_b9a40_row10_col2\" class=\"data row10 col2\" >0.275985</td>\n",
       "      <td id=\"T_b9a40_row10_col3\" class=\"data row10 col3\" >x</td>\n",
       "      <td id=\"T_b9a40_row10_col4\" class=\"data row10 col4\" >x</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x176fe22a0>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merging df_results with df_r2\n",
    "df_merged_r2 = pd.merge(df_results, df_r2, left_on='variable', right_on='Variable', how='left')\n",
    "\n",
    "#Creating column LRM Training R^2 > 0.85\n",
    "df_merged_r2['LRM Training R^2 > 0.85'] = ''\n",
    "#Mark coloumn with x if condition met\n",
    "df_merged_r2.loc[df_merged_r2['R^2 On Training Data'] > 0.85, 'LRM Training R^2 > 0.85'] = 'x'\n",
    "\n",
    "#Identify VAR variables with |r| > 0.5\n",
    "high_corr_vars = set()\n",
    "for v1, v2, corr_val in high_corr_pairs:\n",
    "    high_corr_vars.add(v1)\n",
    "    high_corr_vars.add(v2)\n",
    "\n",
    "#Creating column for VAR correlation condition\n",
    "df_merged_r2['VAR Correlation |r| > 0.5'] = ''\n",
    "#Mark column with x if condition met\n",
    "df_merged_r2.loc[df_merged_r2['variable'].isin(high_corr_vars), 'VAR Correlation |r| > 0.5'] = 'x'\n",
    "\n",
    "final_df = df_merged_r2[['Variable', 'DM_stat', 'p_value', 'LRM Training R^2 > 0.85', 'VAR Correlation |r| > 0.5']]\n",
    "\n",
    "styled = final_df.style.set_table_styles([{'selector': 'th', 'props': [('text-align', 'left')]}]) \\\n",
    "                  .set_properties(**{'text-align': 'left'}) \\\n",
    "                  .hide(axis='index')\n",
    "styled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
