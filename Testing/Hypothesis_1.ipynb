{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bilt = pd.read_csv('Data/de_bilt_weather.csv')\n",
    "\n",
    "# Shift days so that the first day starts from zero\n",
    "df_bilt['days'] = df_bilt['days'] - df_bilt['days'].min()\n",
    "\n",
    "variables = ['cloud_cover', 'wind_speed', 'wind_gust', 'humidity',\n",
    "             'pressure', 'global_radiation', 'precipitation', 'sunshine', \n",
    "             'temp_mean', 'temp_min', 'temp_max'] \n",
    "\n",
    "n_var = len(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training on year of data, testing on 10 days\n",
    "train_range = 365\n",
    "test_range = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data with shape (n_var, train_range+test_range)\n",
    "data = []\n",
    "for variable_name in variables:\n",
    "    data.append(df_bilt[variable_name][:train_range + test_range])\n",
    "data = np.vstack(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting training and test sets\n",
    "train_data = data[:, :train_range]   \n",
    "test_data = data[:, train_range:]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cost function for the optimizer\n",
    "def var_initial_norm(params, data, means, n_var):\n",
    "    c = params[:n_var]\n",
    "    matrix = params[n_var:].reshape(n_var, n_var)\n",
    "    prediction = c[:, None] + matrix @ data[:, :-1]\n",
    "    residuals = data[:, 1:] - prediction\n",
    "    return np.linalg.norm(residuals / means[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize c and M to zero\n",
    "params = np.zeros(n_var + n_var**2)\n",
    "means = np.mean(train_data, axis=1)\n",
    "\n",
    "#Scipy's optimizer fits model parameters by minimizing cost function\n",
    "result = scipy.optimize.minimize(var_initial_norm, params, method='Powell', args=(train_data, means, n_var))\n",
    "\n",
    "#Represents the fitted intercepts and coefficient matrix\n",
    "c_var = result.x[:n_var]\n",
    "M_var = result.x[n_var:].reshape(n_var, n_var)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = train_data[:, 1:] - (c_var[:, None] + M_var @ train_data[:, :-1])\n",
    "\n",
    "#Generating uncertainty in predictions\n",
    "std_var = np.std(residuals, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Produces predictions for next day given today, fits model plus normally distributed noise for uncertainty.\n",
    "def weather_var(x, c, M, std):\n",
    "    return c + M @ x + np.random.normal(0, std)\n",
    "\n",
    "#Time indices for training and testing\n",
    "t_train_data = np.arange(train_range)\n",
    "t_test_data = np.arange(train_range, train_range + test_range)\n",
    "\n",
    "#Number of simulation runs to capture uncertainty\n",
    "prediction_list = []\n",
    "n_predictions = 1000\n",
    "\n",
    "for j in range(n_predictions):\n",
    "    #First test day is initial condition\n",
    "    prediction = [test_data[:, 0]]\n",
    "    #Predict forward for remaining test days\n",
    "    for _ in t_test_data[1:]:\n",
    "        prediction.append(weather_var(prediction[-1], c_var, M_var, std_var))\n",
    "    prediction = np.array(prediction).T\n",
    "    prediction_list.append(prediction)\n",
    "\n",
    "#List of predictions converting to array with shape (n_predictions, n_var, test_range)\n",
    "prediction_matrix = np.array(prediction_list)\n",
    "mean = np.mean(prediction_matrix, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Placing mean of predictions in a DF with variables as columns and days as rows\n",
    "predictions_df = pd.DataFrame(mean.T, columns=variables)\n",
    "\n",
    "#Correlation matrix of the VARs predictions\n",
    "corr_matrix = predictions_df.corr()\n",
    "\n",
    "#Correlation threshold\n",
    "threshold = 0.5\n",
    "\n",
    "# Identifying pairs of variables with high correlation\n",
    "high_corr_pairs = []\n",
    "for i, var1 in enumerate(variables):\n",
    "    for var2 in variables[i + 1:]:\n",
    "        corr_value = corr_matrix.loc[var1, var2]\n",
    "        if abs(corr_value) > threshold:\n",
    "            high_corr_pairs.append((var1, var2, corr_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dict from the test period of days, actual variable, and var_forecast of variable\n",
    "days_test = df_bilt['days'].iloc[train_range:train_range + test_range].values\n",
    "data_dict = {\"days\": days_test}\n",
    "\n",
    "#Adding actual observed values for each variable\n",
    "for i, variable_name in enumerate(variables):\n",
    "    data_dict[f\"actual_{variable_name}\"] = test_data[i, :]\n",
    "\n",
    "#Adding VAR mean predictions for each variable\n",
    "for i, variable_name in enumerate(variables):\n",
    "    data_dict[f\"var_forecast_{variable_name}\"] = mean[i, :]\n",
    "\n",
    "df_var = pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Standalone LRM mean prediciton values \n",
    "files = [\n",
    "    'Testing/LRM_Data/cloud_cover.csv',\n",
    "    'Testing/LRM_Data/global_radiation.csv',\n",
    "    'Testing/LRM_Data/humidity.csv',\n",
    "    'Testing/LRM_Data/precipitation.csv',\n",
    "    'Testing/LRM_Data/pressure.csv',\n",
    "    'Testing/LRM_Data/sunshine.csv',\n",
    "    'Testing/LRM_Data/temp_max.csv',\n",
    "    'Testing/LRM_Data/temp_mean.csv',\n",
    "    'Testing/LRM_Data/temp_min.csv',\n",
    "    'Testing/LRM_Data/wind_gust.csv',\n",
    "    'Testing/LRM_Data/wind_speed.csv',\n",
    "]\n",
    "\n",
    "df_lrm_merged = None\n",
    "#Merging each LRM file on days\n",
    "for i, f in enumerate(files):\n",
    "    df_temp = pd.read_csv(f)\n",
    "    if i == 0:\n",
    "        df_lrm_merged = df_temp\n",
    "    else:\n",
    "        df_lrm_merged = pd.merge(df_lrm_merged, df_temp, on = 'days')\n",
    "\n",
    "#Merging VAR results with df_lrm_merged on days\n",
    "df_merged = pd.merge(df_var, df_lrm_merged, on='days', suffixes=(\"\", \"_lrm\"))\n",
    "\n",
    "#Rename VAR actual columns to standard names\n",
    "rename_dict = {}\n",
    "for var in variables:\n",
    "    old_name = f\"actual_{var}_var\"\n",
    "    new_name = f\"actual_{var}\"\n",
    "    if old_name in df_merged.columns:\n",
    "        rename_dict[old_name] = new_name\n",
    "\n",
    "df_merged.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "#Dropping duplicate actual coloumns\n",
    "for var in variables:\n",
    "    col_to_drop = f\"actual_{var}_lrm\"\n",
    "    if col_to_drop in df_merged.columns:\n",
    "        df_merged.drop(columns=col_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def norm_cdf(z):\n",
    "    #cdf of the standard normal variable at point z using error function (erf)\n",
    "    return 0.5 * (1 + math.erf(z / math.sqrt(2)))\n",
    "\n",
    "def diebold_mariano_test(d, alternative = 'two-sided'):\n",
    "    d = np.array(d)\n",
    "    \n",
    "    #T is the number of observations of loss differentials in the data\n",
    "    T = d.size\n",
    "    d_bar = np.mean(d)\n",
    "    lag = 1\n",
    "\n",
    "    # Compute sample variance of d\n",
    "    var_d = np.sum((d - d_bar)**2) / T\n",
    "\n",
    "    #Dm stat:\n",
    "    dm_stat = d_bar / np.sqrt(var_d / T)\n",
    "\n",
    "    # two-sided test: p-value = 2 * (1 - cdf(|dm_stat|))\n",
    "    if alternative == 'two-sided':\n",
    "        p_value = 2 * (1 - norm_cdf(abs(dm_stat)))\n",
    "        # one-sided \"less\": p-value = cdf(dm_stat)\n",
    "    elif alternative == 'less':\n",
    "        p_value = norm_cdf(dm_stat)\n",
    "    # one-sided \"greater\": p-value = 1 - cdf(dm_stat)\n",
    "    elif alternative == 'greater':\n",
    "        p_value = 1 - norm_cdf(dm_stat)\n",
    "\n",
    "    return dm_stat, p_value\n",
    "\n",
    "#Iterating for each variable\n",
    "results = []\n",
    "for var in variables:\n",
    "    res = run_dm_test_for_variable(df_merged, var)\n",
    "    results.append(res)\n",
    "\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the LRM R^2 file for presentation comparison\n",
    "df_r2 = pd.read_csv(\"Testing/LRM_Data/R_2.csv\", header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging df_results with df_r2\n",
    "df_merged_r2 = pd.merge(df_results, df_r2, left_on='variable', right_on='Variable', how='left')\n",
    "\n",
    "#Creating column LRM Training R^2 > 0.85\n",
    "df_merged_r2['LRM Training R^2 > 0.85'] = ''\n",
    "#Mark coloumn with x if condition met\n",
    "df_merged_r2.loc[df_merged_r2['R^2 On Training Data'] > 0.85, 'LRM Training R^2 > 0.85'] = 'x'\n",
    "\n",
    "#Identify VAR variables with |r| > 0.5\n",
    "high_corr_vars = set()\n",
    "for v1, v2, corr_val in high_corr_pairs:\n",
    "    high_corr_vars.add(v1)\n",
    "    high_corr_vars.add(v2)\n",
    "\n",
    "#Creating column for VAR correlation condition\n",
    "df_merged_r2['VAR Correlation |r| > 0.5'] = ''\n",
    "#Mark column with x if condition met\n",
    "df_merged_r2.loc[df_merged_r2['variable'].isin(high_corr_vars), 'VAR Correlation |r| > 0.5'] = 'x'\n",
    "\n",
    "final_df = df_merged_r2[['Variable', 'DM_stat', 'p_value', 'LRM Training R^2 > 0.85', 'VAR Correlation |r| > 0.5']]\n",
    "\n",
    "styled = final_df.style.set_table_styles([{'selector': 'th', 'props': [('text-align', 'left')]}]) \\\n",
    "                  .set_properties(**{'text-align': 'left'}) \\\n",
    "                  .hide(axis='index')\n",
    "styled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
